{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path if running this file as a main\n",
    "import sys\n",
    "import pathlib\n",
    "p = str(pathlib.Path('.').parent.resolve()) + '/'\n",
    "sys.path.append(p)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    # If not tqdm is not available, provide a mock version of it\n",
    "    def tqdm(iterator, *args, **kwargs):\n",
    "        return iterator\n",
    "\n",
    "import scann\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from src.utils.eval_models import FeatureExtractorBeta as FeatureExtractor\n",
    "from src.utils.eval_models import FeatureExtractorSimple\n",
    "from src.utils.eval_metrics import PredsmIoU\n",
    "\n",
    "from src.utils.transforms import get_hbird_val_transforms, get_hbird_train_transforms, get_hbird_train_transforms_for_imgs\n",
    "\n",
    "from src.utils.image_transformations import CombTransforms, ToFloat\n",
    "from data.VOCdevkit.vocdata_2 import VOCDataModule\n",
    "from data.ade20k.ade20kdata import Ade20kDataModule\n",
    "\n",
    "from src.utils.transforms import get_default_val_transforms\n",
    "\n",
    "# TODO: Include as new imports\n",
    "import torchvision.transforms as trn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('out.txt', 'w')\n",
    "sys.stdout = f\n",
    "# sys.stdout = orig_stdout\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import Tuple, Any\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Dataset(torch.nn.Module):\n",
    "    @abstractmethod\n",
    "    def get_train_loader(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_test_loader(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_val_loader(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_dataset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_test_dataset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_val_dataset(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_num_classes(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_dataset_name(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class NYUDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str = \"train\",\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            transforms: Optional[Callable] = None,\n",
    "            return_masks: bool = False\n",
    "    ):\n",
    "        # super(VOCDataset, self).__init__(root, transforms, transform, target_transform)\n",
    "        super(NYUDataset, self).__init__()\n",
    "        ## check if data is not part of root\n",
    "\n",
    "        if \"data\" not in os.listdir(root):\n",
    "            self.root = os.path.join(root, \"data\")\n",
    "        else:\n",
    "            self.root = root\n",
    "\n",
    "        if image_set == \"train\":\n",
    "            csv_path = os.path.join(root, 'data/nyu2_train.csv')\n",
    "        elif image_set == \"val\":\n",
    "            csv_path = os.path.join(root, 'data/nyu2_test.csv')\n",
    "        \n",
    "        self.image_set = image_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transforms = transforms\n",
    "        self.return_masks = return_masks    \n",
    "        file_paths = pd.read_csv(csv_path)\n",
    "        self.RGB_paths = [os.path.join(self.root, img) for img in file_paths.iloc[:, 0]]\n",
    "        self.depth_paths = [os.path.join(self.root, img) for img in file_paths.iloc[:, 1]]\n",
    "\n",
    "        assert len(self.RGB_paths) == len(self.depth_paths) \n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        # print(self.RGB_paths[index])\n",
    "        # print(self.depth_paths[index])\n",
    "        img = Image.open(self.RGB_paths[index])\n",
    "        depth = Image.open(self.depth_paths[index])\n",
    "        # print('1)',self.depth_paths[index])\n",
    "        # print('2)', self.RGB_paths[index])\n",
    "\n",
    "        if self.image_set == \"val\":\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            if self.transforms:\n",
    "                img, depth = self.transforms(img, depth)\n",
    "            return img, depth\n",
    "        elif \"train\" in self.image_set:\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            if self.transforms:\n",
    "                res = self.transforms(img, depth)\n",
    "                return res\n",
    "            if self.return_masks:\n",
    "                return img, depth\n",
    "            return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.RGB_paths)\n",
    "\n",
    "\n",
    "\n",
    "class NYUDataModule():\n",
    "    \"\"\" \n",
    "    DataModule for Pascal VOC dataset\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): batch size\n",
    "        train_transform (torchvision.transforms): transform for training set\n",
    "        val_transform (torchvision.transforms): transform for validation set\n",
    "        test_transform (torchvision.transforms): transform for test set\n",
    "        dir (str): path to dataset\n",
    "        year (str): year of dataset\n",
    "        split (str): split of dataset\n",
    "        num_workers (int): number of workers for dataloader\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 train_transforms: Optional[Callable],\n",
    "                 batch_size: int,\n",
    "                 num_workers: int,\n",
    "                 train_split: str=\"train\",\n",
    "                 val_split: str=\"val\",\n",
    "                 val_image_transform: Optional[Callable]=None,\n",
    "                 val_target_transform: Optional[Callable]=None,\n",
    "                 val_transforms: Optional[Callable]=None,\n",
    "                 shuffle: bool = True,\n",
    "                 return_masks: bool = False,\n",
    "                 drop_last: bool = False) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.return_masks = return_masks\n",
    "        self.train_transforms = train_transforms\n",
    "        self.val_image_transform = val_image_transform\n",
    "        self.val_target_transform=val_target_transform\n",
    "        self.val_transforms=val_transforms\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = NYUDataset(self.data_dir, self.train_split, transforms=self.train_transforms, return_masks=self.return_masks)\n",
    "        self.val_dataset = NYUDataset(self.data_dir, self.val_split, transform=self.val_image_transform, target_transform=self.val_target_transform, transforms=self.val_transforms, return_masks=True)\n",
    "        # self.test_dataset = NYUDataset(self.dir, self.val_split, transform=self.val_image_transform, target_transform=self.val_target_transform, transforms=self.val_transforms, return_masks=True)\n",
    "\n",
    "    def train_dataloader(self, batch_size=None):\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return DataLoader(self.train_dataset, batch_size=batch_size, shuffle=self.shuffle, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self, batch_size=None):\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        return DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    # def get_test_dataloader(self, batch_size=None):\n",
    "    #     batch_size = self.batch_size if batch_size is None else batch_size\n",
    "    #     return DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "    \n",
    "    def get_train_dataset_size(self):\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def get_val_dataset_size(self):\n",
    "        return len(self.val_dataset)\n",
    "\n",
    "    # def get_test_dataset_size(self):\n",
    "    #     return len(self.test_dataset)\n",
    "\n",
    "    def get_module_name(self):\n",
    "        return \"NYUDataModule\"\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Version 2\n",
    "\n",
    "def rmse(x,y, eps = 1e-6):\n",
    "    return torch.sqrt(torch.nn.functional.mse_loss(x, y) + eps)\n",
    "\n",
    "class HbirdEvaluation():\n",
    "    def __init__(self, feature_extractor, train_loader, n_neighbours, augmentation_epoch, num_classes, device, sampling_fract_num=None,nn_params=None, memory_size=None, \n",
    "                        dataset_size=None, f_mem_p=None, l_mem_p=None, sampled_indices_p=None, sampling_approach=None, is_target_int=False, patch_size=14): # TODO: Change\n",
    "        if nn_params is None:\n",
    "            nn_params = {}\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = device\n",
    "        self.augmentation_epoch = augmentation_epoch\n",
    "        self.memory_size = memory_size\n",
    "        self.n_neighbours = n_neighbours\n",
    "        self.feature_extractor.eval()\n",
    "        self.feature_extractor = feature_extractor.to(self.device)\n",
    "        self.num_classes = num_classes\n",
    "        eval_spatial_resolution = self.feature_extractor.eval_spatial_resolution\n",
    "        self.num_sampled_features = None\n",
    "        self.f_mem_p = f_mem_p\n",
    "        self.l_mem_p = l_mem_p\n",
    "\n",
    "        self.is_target_int = is_target_int\n",
    "        self.sampled_indices_p = sampled_indices_p\n",
    "        self.sampling_fract_num = sampling_fract_num if sampling_fract_num is not None else 1\n",
    "\n",
    "        self.sampled_indices_all = None  # keeps track of the sampled indices of patches per image of the augmented datasets for reproducibility purposes\n",
    "\n",
    "        assert sampling_approach==\"random\" or sampling_approach==\"u_patch_frequency\" or sampling_approach is None # uniformly randomly or the unique patch frequency approach \n",
    "        \n",
    "        self.sampling_patch_fn = None\n",
    "        if sampling_approach == \"u_patch_frequency\":\n",
    "            self.sampling_patch_fn = self.sample_features\n",
    "        elif sampling_approach == \"random\":\n",
    "            self.sampling_patch_fn = self.sample_features_randomly\n",
    "\n",
    "        if self.memory_size is not None:\n",
    "            # define how many sampled patches to get per image\n",
    "            self.num_sampled_features = self.memory_size // (dataset_size * self.augmentation_epoch)\n",
    "            # create memory of specific size for features and labels\n",
    "            self.feature_memory = torch.zeros((self.memory_size, self.feature_extractor.d_model))\n",
    "            self.label_memory = torch.zeros((self.memory_size, self.num_classes if self.num_classes > 0 else patch_size*patch_size)) # TODO: change 1\n",
    "        if self.load_memory() == False:\n",
    "            # if memory was not loaded then try to load the indices of the patches selected from each augmented image\n",
    "            loaded_sample_idces = self.load_sampled_indices()\n",
    "            self.create_memory(train_loader, num_classes=self.num_classes, eval_spatial_resolution=eval_spatial_resolution, sel_sampled_indices=self.sampled_indices_all)\n",
    "            if loaded_sample_idces == False:\n",
    "                # save the sampled indices if they have not already been loaded\n",
    "                self.save_sampled_indices()\n",
    "            self.save_memory()\n",
    "        # Move memory of features and labels to the specified device\n",
    "        self.feature_memory = self.feature_memory.to(self.device)\n",
    "        self.label_memory = self.label_memory.to(self.device)\n",
    "\n",
    "        self.create_NN(self.n_neighbours, **nn_params)\n",
    "    \n",
    "    def create_NN(self, n_neighbours=30, distance_measure=\"dot_product\", num_leaves=512, num_leaves_to_search=32, anisotropic_quantization_threshold=0.2, num_reordering_candidates=120):\n",
    "    # def create_NN(self, n_neighbours=30, distance_measure=\"dot_product\", num_leaves=128, num_leaves_to_search=32, anisotropic_quantization_threshold=0.2, num_reordering_candidates=32):\n",
    "        self.NN_algorithm = scann.scann_ops_pybind.builder(self.feature_memory.detach().cpu().numpy(), n_neighbours, distance_measure).tree(\n",
    "    num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search, training_sample_size=self.feature_memory.size(0)).score_ah(\n",
    "    2, anisotropic_quantization_threshold=anisotropic_quantization_threshold).reorder(num_reordering_candidates).build()\n",
    "\n",
    "    def create_memory(self, train_loader, num_classes, eval_spatial_resolution, sel_sampled_indices=None):\n",
    "        feature_memory = list()\n",
    "        label_memory = list()\n",
    "        idx = 0\n",
    "        sampled_indices_all = list()\n",
    "        bs_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for j in tqdm(range(self.augmentation_epoch), desc='Augmentation loop'):\n",
    "                for i, (x, y) in enumerate(tqdm(train_loader, desc='Memory Creation loop')):\n",
    "                    print('x', x.shape)\n",
    "                    print('y', y.shape)\n",
    "                    input_size = x.shape[-1]\n",
    "                    self.patch_size = input_size // eval_spatial_resolution\n",
    "                    # Step 1: Load Images and Target Labels\n",
    "                    bs = x.shape[0]\n",
    "                    x = x.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "                    # print('train y',y)\n",
    "                    # Step 2: Preprocess the target labels\n",
    "                    ## Transform floating point labels between [0,1] to integers between [0,255]\n",
    "                    if self.is_target_int == True:\n",
    "                        y = (y * 255).long()\n",
    "                        ## ignore the label of 255\n",
    "                        y[y == 255] = 0\n",
    "                    # Step 3: Encode images\n",
    "                    features, _ = self.feature_extractor.forward_features(x) # features of shape (bs, spatial_resolution*spatial_resolution, d)\n",
    "                    # Step 4: Patchfify target labels to have the same shape as the features\n",
    "                    patchified_gts = self.patchify_gt(y, patch_size) # of shape (bs, spatial_resolution, spatial_resolution, c*patch_size*patch_size)\n",
    "                    if self.is_target_int == True:\n",
    "                        # Convert integer labels to one hot encoding\n",
    "                        patch_gts = F.one_hot(patchified_gts, num_classes=num_classes).float() # of shape (bs, spatial_resolution, spatial_resolution, c*patch_size*patch_size, n_classes)\n",
    "                        label = patch_gts.mean(dim=3) # of shape (bs, spatial_resolution, spatial_resolution, n_classes) # TODO: Change\n",
    "                    else:\n",
    "                        label = patchified_gts # of shape (bs, spatial_resolution, spatial_resolution, c*patch_size*patch_size) # TODO: Change\n",
    "                    \n",
    "                    if self.memory_size is None or self.sampling_patch_fn is None:\n",
    "                        # Memory Size is unbounded so we store all the features\n",
    "                        normalized_features = features / torch.norm(features, dim=1, keepdim=True)\n",
    "\n",
    "                        normalized_features = normalized_features.flatten(0, 1)\n",
    "                        label = label.flatten(0, 2)\n",
    "                        feature_memory.append(normalized_features.detach().cpu())\n",
    "                        label_memory.append(label.detach().cpu())\n",
    "                    else:\n",
    "                        for i in tqdm(range(0, self.sampling_fract_num), desc='Sampling loop'):\n",
    "                            if sel_sampled_indices is None:\n",
    "                                # Memory Size is bounded so we need to select/sample some features only\n",
    "                                # sampled_features, sampled_indices = self.sample_features(features, patchified_gts)\n",
    "                                # sampled_features, sampled_indices = self.sample_features_randomly(features, patchified_gts)\n",
    "                                # print('features',features.shape)\n",
    "                                # print('patchified_gts',patchified_gts.shape)\n",
    "                                sampled_features, sampled_indices = self.sampling_patch_fn(features, patchified_gts)\n",
    "                                sampled_indices_all.append(sampled_indices.detach().cpu())\n",
    "                            else:\n",
    "                                sampled_indices = sel_sampled_indices[bs_idx*bs: (bs_idx+1)*bs]\n",
    "                                sampled_features = torch.gather(features, 1, sampled_indices.unsqueeze(-1).repeat(1, 1, features.shape[-1]).to(self.device))\n",
    "\n",
    "                            normalized_sampled_features = sampled_features / torch.norm(sampled_features, dim=1, keepdim=True)\n",
    "                            # combine the dimensions of patches across h and w \n",
    "                            label_hat = label.flatten(1, 2)\n",
    "                            ## select the labels of the sampled features\n",
    "                            sampled_indices = sampled_indices.to(self.device)\n",
    "                            print('sampled_indices',sampled_indices.shape)\n",
    "                            # print('sampled_indices',sampled_indices)\n",
    "                            print('label',label.shape)\n",
    "                            # print('label',label)\n",
    "                            print('label_hat',label_hat.shape)\n",
    "                            print('feature_memory',self.feature_memory.shape)\n",
    "                            print('label_memory',self.label_memory.shape)\n",
    "                            print('sampled_indices.unsqueeze(-1).repeat(1, 1, label.shape[-1])',sampled_indices.unsqueeze(-1).repeat(1, 1, label.shape[-1]).shape)\n",
    "                            label_hat = label_hat.gather(1, sampled_indices.unsqueeze(-1).repeat(1, 1, label.shape[-1]))\n",
    "\n",
    "                            normalized_sampled_features = normalized_sampled_features.flatten(0, 1)\n",
    "                            label_hat = label_hat.flatten(0, 1)\n",
    "                            self.feature_memory[idx:idx+normalized_sampled_features.size(0)] = normalized_sampled_features.detach().cpu()\n",
    "                            self.label_memory[idx:idx+label_hat.size(0)] = label_hat.detach().cpu()\n",
    "                            idx += normalized_sampled_features.size(0)\n",
    "                            bs_idx += 1\n",
    "            if self.memory_size is None or self.sampling_patch_fn is None:\n",
    "                self.feature_memory = torch.cat(feature_memory)\n",
    "                self.label_memory = torch.cat(label_memory)\n",
    "            \n",
    "            if sel_sampled_indices is None and self.memory_size is not None:\n",
    "                self.sampled_indices_all = torch.cat(sampled_indices_all)\n",
    "            else:\n",
    "                self.sampled_indices_all = sel_sampled_indices\n",
    "\n",
    "    def save_memory(self):\n",
    "        if self.f_mem_p is not None:\n",
    "            torch.save(self.feature_memory.cpu(), self.f_mem_p)\n",
    "        if self.l_mem_p is not None:\n",
    "            torch.save(self.label_memory.cpu(), self.l_mem_p)\n",
    "\n",
    "    def load_memory(self):\n",
    "        if self.f_mem_p is not None and self.l_mem_p is not None and os.path.isfile(self.f_mem_p) and os.path.isfile(self.l_mem_p):\n",
    "            self.feature_memory = torch.load(self.f_mem_p).to(self.device)\n",
    "            self.label_memory = torch.load(self.l_mem_p).to(self.device)\n",
    "            print('Memory features loaded from', self.f_mem_p)\n",
    "            print('Memory labels from', self.l_mem_p)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_sampled_indices(self):\n",
    "        if self.sampled_indices_p is not None and self.sampled_indices_all is not None:\n",
    "            torch.save(self.sampled_indices_all.cpu(), self.sampled_indices_p)\n",
    "            print('Sampled indices save to', self.sampled_indices_p)\n",
    "\n",
    "    def load_sampled_indices(self):\n",
    "        if self.sampled_indices_p is not None and os.path.isfile(self.sampled_indices_p):\n",
    "            self.sampled_indices_all = torch.load(self.sampled_indices_p, map_location='cpu')\n",
    "            print('Sampled indices loaded from', self.sampled_indices_p)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def sample_features(self, features, pathified_gts):\n",
    "        sampled_features = []\n",
    "        sampled_indices = []\n",
    "        for k, gt in enumerate(tqdm(pathified_gts)):\n",
    "            class_frequency = self.get_class_frequency(gt)\n",
    "            patch_scores, nonzero_indices = self.get_patch_scores(gt, class_frequency)\n",
    "\n",
    "            patch_scores = patch_scores.flatten()\n",
    "            nonzero_indices = nonzero_indices.flatten()\n",
    "\n",
    "            # assert zero_score_idx[0].size(0) != 0 ## for pascal every patch should belong to one class\n",
    "            patch_scores[~nonzero_indices] = 1e6\n",
    "\n",
    "            # sample uniform distribution with the same size as the\n",
    "            # number of nonzero indices (we use the sum here as the\n",
    "            # nonzero_indices matrix is a boolean mask)\n",
    "            uniform_x = torch.rand(nonzero_indices.sum())\n",
    "            patch_scores[nonzero_indices] *= uniform_x\n",
    "            feature = features[k]\n",
    "            ### select the least num_sampled_features score idndices\n",
    "            # print('patch_scores',patch_scores.shape)\n",
    "            # print('self.num_sampled_features',self.num_sampled_features)\n",
    "            _, indices = torch.topk(patch_scores, self.num_sampled_features, largest=False)\n",
    "            sampled_indices.append(indices)\n",
    "            samples = feature[indices]\n",
    "            sampled_features.append(samples)\n",
    "        sampled_features = torch.stack(sampled_features)\n",
    "        sampled_indices = torch.stack(sampled_indices)\n",
    "        return sampled_features, sampled_indices\n",
    "\n",
    "    def sample_features_randomly(self, features, pathified_gts):\n",
    "        bs, ps, d = features.shape\n",
    "        # print('IN sample_features_randomly: features',features.shape)\n",
    "        # print('IN sample_features_randomly: pathified_gts',pathified_gts.shape)\n",
    "        # print('IN sample_features_randomly: self.num_sampled_features',self.num_sampled_features)\n",
    "        sampled_indices = torch.argsort(torch.rand((bs, ps), device=self.device), dim=1)[:,:self.num_sampled_features]\n",
    "        # print('IN sample_features_randomly: sampled_indices',sampled_indices.shape)\n",
    "        sampled_features = torch.gather(features, 1, sampled_indices.unsqueeze(-1).repeat(1,1,d))\n",
    "        # print('IN sample_features_randomly: sampled_features',sampled_features.shape)\n",
    "        return sampled_features, sampled_indices\n",
    "\n",
    "    def get_class_frequency(self, gt):\n",
    "        class_frequency = torch.zeros((self.num_classes), device=self.device)\n",
    "\n",
    "        for i in range(gt.shape[0]):\n",
    "            for j in range(gt.shape[1]):\n",
    "                patch_classes = gt[i, j].unique()\n",
    "                class_frequency[patch_classes] += 1\n",
    "\n",
    "        return class_frequency\n",
    "\n",
    "    def get_patch_scores(self, gt, class_frequency):\n",
    "        patch_scores = torch.zeros((gt.shape[0], gt.shape[1]))\n",
    "        nonzero_indices = torch.zeros((gt.shape[0], gt.shape[1]), dtype=torch.bool)\n",
    "\n",
    "        for i in range(gt.shape[0]):\n",
    "            for j in range(gt.shape[1]):\n",
    "                patch_classes = gt[i, j].unique()\n",
    "                patch_scores[i, j] = class_frequency[patch_classes].sum()\n",
    "                nonzero_indices[i, j] = patch_classes.shape[0] > 0\n",
    "\n",
    "        return patch_scores, nonzero_indices\n",
    "\n",
    "    def patchify_gt(self, gt, patch_size):\n",
    "        bs, c, h, w = gt.shape\n",
    "        gt = gt.reshape(bs, c, h//patch_size, patch_size, w//patch_size, patch_size)\n",
    "        gt = gt.permute(0, 2, 4, 1, 3, 5)\n",
    "        gt = gt.reshape(bs, h//patch_size, w//patch_size, c*patch_size*patch_size)\n",
    "        return gt\n",
    "    \n",
    "    def unpacthify_gt(self, gt, patch_size):\n",
    "        bs, h, w, c = gt.shape\n",
    "        gt = gt.reshape(bs, h, w, c//patch_size, patch_size, patch_size)\n",
    "        gt = gt.permute(0, 3, 1, 4, 2, 5)\n",
    "        gt = gt.reshape(bs, c//patch_size, h*patch_size, w*patch_size)\n",
    "        return gt\n",
    "\n",
    "    def cross_attention(self, q, k, v, beta=0.02):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            q (torch.Tensor): query tensor of shape (bs, num_patches, d_k)\n",
    "            k (torch.Tensor): key tensor of shape (bs, num_patches,  NN, d_k)\n",
    "            v (torch.Tensor): value tensor of shape (bs, num_patches, NN, label_dim)\n",
    "        \"\"\"\n",
    "        d_k = q.size(-1)\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "        q = q.unsqueeze(2) ## (bs, num_patches, 1, d_k)\n",
    "        attn = torch.einsum(\"bnld,bnmd->bnlm\", q, k) / beta\n",
    "        attn = attn.squeeze(2)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = attn.unsqueeze(-1)\n",
    "        label_hat = torch.einsum(\"blms,blmk->blsk\", attn, v)\n",
    "        label_hat = label_hat.squeeze(-2)\n",
    "        return label_hat\n",
    "    \n",
    "    def find_nearest_key_to_query(self, q):\n",
    "        bs, num_patches, d_k = q.shape\n",
    "        reshaped_q = q.reshape(bs*num_patches, d_k)\n",
    "        neighbors, distances = self.NN_algorithm.search_batched(reshaped_q)\n",
    "        neighbors = neighbors.astype(np.int64)\n",
    "        neighbors = torch.from_numpy(neighbors).to(self.device)\n",
    "        neighbors = neighbors.flatten()\n",
    "        key_features = self.feature_memory[neighbors]\n",
    "        key_features = key_features.reshape(bs, num_patches, self.n_neighbours, -1)\n",
    "        key_labels = self.label_memory[neighbors]\n",
    "        key_labels = key_labels.reshape(bs, num_patches, self.n_neighbours, -1)\n",
    "        return key_features, key_labels\n",
    "\n",
    "    def evaluate(self, val_loader, eval_spatial_resolution, return_knn_details=False, ignore_index=255):\n",
    "        metric = PredsmIoU(self.num_classes, self.num_classes)\n",
    "        self.feature_extractor = self.feature_extractor.to(self.device)\n",
    "        label_hats = []\n",
    "        lables = []\n",
    "        # print('feature_memory',self.feature_memory)\n",
    "        # print('label_memory',self.label_memory)\n",
    "        # print('*'*100)\n",
    "        knns = []\n",
    "        knns_labels = []\n",
    "        knns_ca_labels = []\n",
    "        idx = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(tqdm(val_loader, desc='Evaluation loop')):\n",
    "                # Step 1: Load images to the specified device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                print('eval y',y.shape)\n",
    "                _, _, h, w = x.shape\n",
    "                if self.is_target_int == True:\n",
    "                    # Step 1-b: Transform floating point labels between [0,1] to integers between [0,255]\n",
    "                    y = (y * 255).long()\n",
    "                # Step 3: Encode images\n",
    "                features, _ = self.feature_extractor.forward_features(x.to(self.device)) # features of shape (bs, eval_spatial_reso**2, d)\n",
    "                features = features.to(self.device) \n",
    "                ## copy the data of features to another variable\n",
    "                q = features.clone()\n",
    "                q = q.detach().cpu().numpy()\n",
    "                # Step 4: Find k-NNs to each encoded patch\n",
    "                key_features, key_labels = self.find_nearest_key_to_query(q)  # key_features of shape (bs, eval_spatial_reso**2, 30, d)\n",
    "                # key_labels of shape (bs, eval_spatial_reso**2, 30, n_classes)\n",
    "                # print('key_features',key_features.shape)  \n",
    "                # print('key_labels',key_labels.shape)  \n",
    "                # print('feature_memory',self.feature_memory.shape)\n",
    "                # print('label_memory',self.label_memory.shape)\n",
    "                # Step 5: Apply cross-attention on the k retrieved neighbors\n",
    "                label_hat =  self.cross_attention(features, key_features, key_labels)\n",
    "                ## \n",
    "                if return_knn_details:\n",
    "                    knns.append(key_features.detach().cpu())\n",
    "                    knns_labels.append(key_labels.detach().cpu())\n",
    "                    knns_ca_labels.append(label_hat.detach().cpu())\n",
    "                bs, _, label_dim = label_hat.shape\n",
    "                label_hat = label_hat.reshape(bs, eval_spatial_resolution, eval_spatial_resolution, label_dim).permute(0, 3, 1, 2)\n",
    "                # print('\\n\\n','-'*40)\n",
    "                # print('label_hat',label_hat)\n",
    "                print('label_hat', label_hat.shape)\n",
    "                # print()\n",
    "                # resized_label_hats =  F.interpolate(label_hat.float(), size=(h, w), mode=\"bilinear\") # TODO: Change\n",
    "                \n",
    "                # print('resized_label_hats',resized_label_hats.shape)\n",
    "                # print('resized_label_hats',resized_label_hats)\n",
    "                if self.is_target_int == True:\n",
    "                    resized_label_hats =  F.interpolate(label_hat.float(), size=(h, w), mode=\"bilinear\") # TODO: change\n",
    "                    print('resized_label_hats',resized_label_hats.shape)\n",
    "                    gt_map = resized_label_hats.argmax(dim=1).unsqueeze(1)\n",
    "                else:\n",
    "                    gt_map = label_hat\n",
    "                    y = self.patchify_gt(y, self.patch_size) # [bs, sres, sres, c*ps*ps]\n",
    "                    gt_map = gt_map.permute(0, 2, 3, 1) # [bs, sres, sres, c*ps*ps]\n",
    "                    print('label_hat', label_hat.shape)\n",
    "                    print('Patchified y', y.shape)\n",
    "                # print('gt_map',gt_map.shape)\n",
    "                label_hats.append(gt_map.detach().cpu())\n",
    "                lables.append(y.detach().cpu())\n",
    "                idx += x.size(0)\n",
    "                # if i >=3:\n",
    "                #     break\n",
    "\n",
    "            lables = torch.cat(lables) \n",
    "            label_hats = torch.cat(label_hats)\n",
    "            # print('\\n\\n','+'*60)\n",
    "            # print('lables',lables.shape)\n",
    "            # print('lables',lables)\n",
    "            # print('_'*20)\n",
    "            # print('label_hats',label_hats.shape)\n",
    "            # print('label_hats',label_hats)\n",
    "            # filter out labels that should not be checked\n",
    "            if self.is_target_int == True:\n",
    "                valid_idx = lables != ignore_index\n",
    "                valid_target = lables[valid_idx]\n",
    "                valid_cluster_maps = label_hats[valid_idx]\n",
    "                # compute miou between ground truth labels and inferred labels\n",
    "                metric.update(valid_target, valid_cluster_maps)\n",
    "                res, tp, fp, fn, reordered_preds, matched_bg_clusters = metric.compute(is_global_zero=True)\n",
    "            else:\n",
    "                res = rmse(lables, label_hats)\n",
    "                print('res', res)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if return_knn_details:\n",
    "                knns = torch.cat(knns)\n",
    "                knns_labels = torch.cat(knns_labels)\n",
    "                knns_ca_labels = torch.cat(knns_ca_labels)\n",
    "                return res, {\"knns\": knns, \"knns_labels\": knns_labels, \"knns_ca_labels\": knns_ca_labels}\n",
    "            else:\n",
    "                return res\n",
    "    \n",
    "\n",
    "def hbird_evaluation(model, d_model, patch_size, dataset_name:str, data_dir:str, batch_size=64, \n",
    "                    input_size=224, augmentation_epoch=1, device='cpu', return_knn_details=False, \n",
    "                    n_neighbours=30, nn_params=None, ftr_extr_fn=None, memory_size=None, num_workers=8, \n",
    "                    eval_dir=None, ignore_index=255, method:str=None, sampling_fract_num=None, seed=400):\n",
    "    eval_spatial_resolution = input_size // patch_size\n",
    "    if ftr_extr_fn is None:\n",
    "        feature_extractor = FeatureExtractor(model, eval_spatial_resolution=eval_spatial_resolution, d_model=d_model)\n",
    "    else:\n",
    "        feature_extractor = FeatureExtractorSimple(model, ftr_extr_fn=ftr_extr_fn, eval_spatial_resolution=eval_spatial_resolution, d_model=d_model)\n",
    "    train_transforms_dict = get_hbird_train_transforms(input_size, n_views=1)\n",
    "    val_transforms_dict = get_hbird_val_transforms(input_size)\n",
    "\n",
    "    train_transforms = CombTransforms(img_transform=train_transforms_dict['img'], tgt_transform=None, img_tgt_transform=train_transforms_dict['shared'])\n",
    "    val_transforms = CombTransforms(img_transform=val_transforms_dict['img'], tgt_transform=None, img_tgt_transform=val_transforms_dict['shared'])\n",
    "    sampling_approach=None\n",
    "    is_target_int=False\n",
    "    dataset_size = 0\n",
    "    num_classes = 0\n",
    "    ignore_index = -1   \n",
    "    if dataset_name == \"voc\":\n",
    "        ignore_index = 255\n",
    "        image_set = \"trainaug\"\n",
    "        if sampling_fract_num is not None:\n",
    "            image_set = f'{image_set}_{sampling_fract_num}_{seed}'\n",
    "        dataset = VOCDataModule(batch_size=batch_size,\n",
    "                                    num_workers=num_workers,\n",
    "                                    train_split=image_set,\n",
    "                                    val_split=\"val\",\n",
    "                                    data_dir=data_dir,\n",
    "                                    train_image_transform=train_transforms,\n",
    "                                    val_transforms=val_transforms,\n",
    "                                    shuffle=False,\n",
    "                                    return_masks=True)\n",
    "        dataset_size = 10582\n",
    "        dataset.setup()\n",
    "        sampling_approach = \"u_patch_frequency\"\n",
    "        is_target_int = True\n",
    "    elif dataset_name == \"ade20k\":\n",
    "        ignore_index = 0\n",
    "        file_set = None\n",
    "        image_set = 'training'\n",
    "        if image_set is not None and sampling_fract_num is not None:\n",
    "            file_set_file = os.path.join(\"/mnt/beegfs/vpariz01/workspace/sets_ade20k\", f'{image_set}_{sampling_fract_num}_{seed}.txt')\n",
    "            with open(file_set_file, \"r\") as f:\n",
    "                file_set = [x.strip() for x in f.readlines()]\n",
    "        dataset = Ade20kDataModule(data_dir,\n",
    "                 train_transforms=train_transforms,\n",
    "                 val_transforms=val_transforms,\n",
    "                 shuffle=False,\n",
    "                 num_workers=num_workers,\n",
    "                 batch_size=batch_size,\n",
    "                 train_file_set=file_set)\n",
    "        dataset_size = 20210\n",
    "        dataset.setup()\n",
    "        sampling_approach = \"u_patch_frequency\"\n",
    "        is_target_int = True\n",
    "    elif dataset_name == \"nyuv2\":\n",
    "        train_transforms_dict = get_default_val_transforms(input_size)\n",
    "        val_transforms_dict = get_default_val_transforms(input_size)\n",
    "\n",
    "        min_val=713.0\n",
    "        max_val=9986.0\n",
    "        tgt_transform_val = trn.Compose([trn.Resize((input_size, input_size)), trn.ToTensor(), ToFloat(), trn.Normalize(mean= [min_val], std= [max_val-min_val])])\n",
    "        # tgt_transform_val = trn.Compose([trn.Resize((input_size, input_size)), trn.ToTensor(), ToFloat()])\n",
    "        train_transforms = CombTransforms(img_transform=train_transforms_dict['img'], tgt_transform=None, img_tgt_transform=train_transforms_dict['shared'])\n",
    "        val_transforms = CombTransforms(img_transform=val_transforms_dict['img'], tgt_transform=tgt_transform_val, img_tgt_transform=train_transforms_dict['shared'])\n",
    "        dataset = NYUDataModule(data_dir,\n",
    "                 train_transforms=train_transforms,\n",
    "                 val_transforms=val_transforms,\n",
    "                 shuffle=False,\n",
    "                 num_workers=num_workers,\n",
    "                 batch_size=batch_size)\n",
    "        dataset.setup()\n",
    "        dataset_size = dataset.get_train_dataset_size()\n",
    "        # val_dataset_size = dataset.get_val_dataset_size()\n",
    "        # num_classes=dataset.get_num_classes()\n",
    "        sampling_approach = \"random\"\n",
    "        is_target_int = False\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    # dataset_size = dataset.get_train_dataset_size()\n",
    "    num_classes = dataset.get_num_classes()\n",
    "    train_loader = dataset.train_dataloader()\n",
    "    val_loader = dataset.val_dataloader()\n",
    "    sampled_indices_p = None\n",
    "    f_mem_p = None\n",
    "    l_mem_p = None\n",
    "    if eval_dir is not None:\n",
    "        if not os.path.exists(eval_dir):\n",
    "            os.makedirs(eval_dir)\n",
    "        sampled_indices_p = os.path.join(eval_dir, f'sampled_indices_{dataset_name}_ms_{memory_size}_ps_{patch_size}_aug_{augmentation_epoch}.pth')\n",
    "    evaluator = HbirdEvaluation(feature_extractor, train_loader, n_neighbours=n_neighbours, \n",
    "            augmentation_epoch=augmentation_epoch, num_classes=num_classes, device=device, \n",
    "            nn_params=nn_params, memory_size=memory_size, dataset_size=dataset_size, \n",
    "            sampled_indices_p=sampled_indices_p, f_mem_p=f_mem_p, l_mem_p=l_mem_p, \n",
    "            sampling_fract_num=None, sampling_approach=sampling_approach,\n",
    "            is_target_int=is_target_int)\n",
    "    miou = evaluator.evaluate(val_loader, eval_spatial_resolution, return_knn_details=return_knn_details, ignore_index=ignore_index)\n",
    "    # print('hbird mIoU:', miou)\n",
    "    return miou\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model dino\n",
    "device = 'cuda'\n",
    "input_size = 518\n",
    "batch_size = 24\n",
    "patch_size = 14\n",
    "embed_dim = 384\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "\n",
    "# Define the function to extract features from the model\n",
    "# Input to the function is the model and the images\n",
    "# Output of the function is the features extracted from the model \n",
    "# and optionally the attention maps\n",
    "# fn = lambda model, imgs: (model.get_intermediate_layers(imgs)[0][:, 1:], None)\n",
    "\n",
    "# device = 'cuda'\n",
    "# input_size = 224\n",
    "# batch_size = 256\n",
    "# patch_size = 14\n",
    "# embed_dim = 384\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "\n",
    "# Define the function to extract features from the model\n",
    "# Input to the function is the model and the images\n",
    "# Output of the function is the features extracted from the model \n",
    "# and optionally the attention maps\n",
    "fn = lambda model, imgs: (model.forward_features(imgs)['x_norm_patchtokens'], None)\n",
    "\n",
    "# Evaluate the model using the Full In-Context Learning Hummingbird  \n",
    "# or Dense k-NN Retrieval Evaluation on the Pascal VOC Dataset\n",
    "hbird_miou = hbird_evaluation(model.to(device), \n",
    "        d_model=embed_dim,          # size of the embedding feature vectors of patches\n",
    "        patch_size=patch_size, \n",
    "        batch_size = batch_size, \n",
    "        input_size=input_size,             \n",
    "        augmentation_epoch=1,       # how many iterations of augmentations to use on top of \n",
    "                                    # the training dataset in order to generate the memory\n",
    "        device=device,              \n",
    "        return_knn_details=False,   # whether to return additional NNs details\n",
    "        n_neighbours=30,           # the number of neighbors to fetch per image patch\n",
    "        nn_params=None,             # Other parameters to be used for the k-NN operator\n",
    "        ftr_extr_fn=fn,             # function that extracts image patch features with \n",
    "                                    # a vision encoder\n",
    "        # dataset_name='voc',         # the name of the dataset to use, \n",
    "                                    # currently only Pascal VOC is included.\n",
    "        dataset_name=\"nyuv2\",\n",
    "        sampling_fract_num=None,\n",
    "        # data_dir='/mnt/beegfs/vpariz01/workspace/datasets/pascal_voc_aug',    # path to the dataset \n",
    "        data_dir = \"/mnt/beegfs/vpariz01/workspace/datasets/nyu-depth-v2/nyu_data\",\n",
    "                                                            # to use for evaluation\n",
    "        # memory_size=1024000,\n",
    "        memory_size=102400,\n",
    "        )           # How much you want to limit your dataset, \n",
    "                                    # None if to be left unbounded\n",
    "print('Dense NN Ret - miou score:', hbird_miou) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obdet_hb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
